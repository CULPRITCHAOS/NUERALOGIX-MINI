# MS MARCO Real Data Benchmark

This directory contains scripts for running a real MS MARCO retrieval benchmark to evaluate boundary-aware compression.

## Scripts

### 1. `msmarco_prepare_subset.py`
Downloads and prepares a real MS MARCO passage subset.
- Downloads MS MARCO v1.1 from Hugging Face
- Selects a subset of passages and queries with relevance labels
- Saves to `data/msmarco_subset/`

**Usage:**
```bash
python analysis/msmarco_prepare_subset.py
python analysis/msmarco_prepare_subset.py --num-passages 50000 --num-queries 1000
```

### 2. `msmarco_embed.py`
Generates real embeddings using a neural network model.
- Uses sentence-transformers/all-MiniLM-L6-v2
- Processes in batches to avoid OOM
- Saves embeddings as .npy files

**Usage:**
```bash
python analysis/msmarco_embed.py
python analysis/msmarco_embed.py --batch-size 32 --model sentence-transformers/all-MiniLM-L6-v2
```

### 3. `msmarco_run_compression.py`
Runs baseline and boundary-aware compression.
- Baseline: lattice-hybrid (k-means + grid)
- Boundary-aware: differential treatment for boundary vectors
- Measures compression time
- Saves compressed embeddings

**Usage:**
```bash
python analysis/msmarco_run_compression.py
python analysis/msmarco_run_compression.py --grid 0.1 --k 10
```

### 4. `msmarco_eval_retrieval.py`
Evaluates retrieval quality with compressed embeddings.
- Computes Recall@10, Recall@100, MRR, NDCG@10
- Uses real relevance judgments from MS MARCO
- Measures query latency
- Saves metrics for both baseline and boundary-aware modes

**Usage:**
```bash
python analysis/msmarco_eval_retrieval.py --mode both
python analysis/msmarco_eval_retrieval.py --mode baseline
python analysis/msmarco_eval_retrieval.py --mode boundary
```

### 5. `msmarco_run_pipeline.py`
Orchestrates the complete pipeline.
- Runs all steps in sequence
- Generates final report with metrics comparison
- Handles failures gracefully (NO simulation fallback)

**Usage:**
```bash
python analysis/msmarco_run_pipeline.py
```

## Requirements

Install dependencies:
```bash
pip install -r analysis/requirements.txt
```

Required packages:
- `datasets` - for downloading MS MARCO from Hugging Face
- `sentence-transformers` - for generating embeddings
- `torch` - PyTorch backend for transformers
- `scikit-learn` - for k-means clustering
- `numpy`, `scipy`, `tqdm`, `matplotlib`

## NO SIMULATION POLICY

**CRITICAL:** These scripts follow a strict NO SIMULATION policy:

- âœ… Use real MS MARCO data from Hugging Face
- âœ… Use real neural network models for embeddings
- âœ… Compute real metrics from actual retrieval results
- âŒ NO synthetic "MS MARCO-like" data
- âŒ NO fabricated metrics
- âŒ NO simulated results

**If the benchmark cannot be run** (e.g., network issues, resource constraints), the pipeline will:
1. Clearly state the failure reason
2. Generate a failure report in `docs/MSMARCO_REAL_DATA_VERDICT.md`
3. **NOT** fall back to simulation or fabrication

## Pipeline Flow

```
1. msmarco_prepare_subset.py
   â†“
   Downloads MS MARCO â†’ saves to data/msmarco_subset/
   
2. msmarco_embed.py
   â†“
   Loads passages/queries â†’ generates embeddings â†’ saves .npy files
   
3. msmarco_run_compression.py
   â†“
   Loads embeddings â†’ runs baseline & boundary compression â†’ saves to results/msmarco/
   
4. msmarco_eval_retrieval.py
   â†“
   Loads compressed embeddings â†’ evaluates retrieval â†’ saves metrics
   
5. msmarco_run_pipeline.py
   â†“
   Orchestrates all steps â†’ generates final report in docs/MSMARCO_REAL_DATA_VERDICT.md
```

## Output Structure

```
data/msmarco_subset/
â”œâ”€â”€ passages.jsonl           # MS MARCO passages
â”œâ”€â”€ queries.jsonl            # MS MARCO queries
â”œâ”€â”€ qrels.tsv                # Relevance judgments
â”œâ”€â”€ passages_embeddings.npy  # Passage embeddings
â”œâ”€â”€ queries_embeddings.npy   # Query embeddings
â”œâ”€â”€ meta.json                # Embedding metadata
â””â”€â”€ dataset_info.json        # Dataset statistics

results/msmarco/
â”œâ”€â”€ baseline/
â”‚   â”œâ”€â”€ compressed_passages.npy  # Baseline compressed embeddings
â”‚   â”œâ”€â”€ compression_info.json    # Compression metadata
â”‚   â”œâ”€â”€ metrics.json             # Retrieval metrics
â”‚   â””â”€â”€ perf.json                # Performance metrics
â”œâ”€â”€ boundary/
â”‚   â”œâ”€â”€ compressed_passages.npy  # Boundary-aware compressed embeddings
â”‚   â”œâ”€â”€ compression_info.json    # Compression metadata
â”‚   â”œâ”€â”€ metrics.json             # Retrieval metrics
â”‚   â””â”€â”€ perf.json                # Performance metrics
â””â”€â”€ run_config.json              # Overall run configuration

docs/
â””â”€â”€ MSMARCO_REAL_DATA_VERDICT.md  # Final report with verdict
```

## Example Output

When successful, `MSMARCO_REAL_DATA_VERDICT.md` will contain:
- Dataset details (num passages, queries)
- Embedding model information
- Compression parameters
- Metrics comparison table (Recall@10, Recall@100, MRR, NDCG@10)
- Performance overhead analysis
- Final verdict: âœ… EXPLOITABLE / ğŸŸ¡ OBSERVATIONAL / âŒ REJECTED

When failed (e.g., network issues), it will contain:
- Clear failure statement
- Exact error message
- What was attempted
- Why it failed
- NO fabricated results

## Resource Requirements

- **Disk space**: ~2-3GB for dataset and embeddings
- **RAM**: ~4-8GB (for embedding generation)
- **Time**: ~30-60 minutes for full pipeline
- **Network**: Required for downloading MS MARCO and model weights
- **GPU**: Optional (speeds up embedding generation)

## Troubleshooting

**Network errors:**
- Ensure access to `huggingface.co`
- Check firewall/proxy settings

**Out of memory:**
- Reduce `--num-passages` or `--num-queries`
- Reduce `--batch-size` for embedding generation

**Slow execution:**
- Use GPU if available
- Reduce dataset size
- Increase `--batch-size` if you have enough RAM
